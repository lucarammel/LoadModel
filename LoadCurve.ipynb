{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model manager of the Load Curve modelling \n",
    "\n",
    "This notebook is intented to be a user-friendly approach of the **load curve model**. Just run the steps you need. \n",
    "## 1 - Make dataset \n",
    "This steps is the whole preprocessing pipeline of the data. From the download of weather data, the preparation of annual demands, hourly load data to the split into train set and test set. Inputs to provide during the run : **split_week_end** (e.g whether you want to split week end in saturday and sunday) , **not_use_wem_inputs** (whether you want to use WEM inputs hourly load by subsector for the training. If False, the model uses the annual demand of subsector only. There is just one phase of training.), **pop_weighted** (e.g whether you want the WEM inputs hourly_load disaggregation using population averaged or using annual demand)\n",
    "\n",
    "## 2 - Train model\n",
    "\n",
    "This is the train model step. The model architecture is the following : **The model estimates the hourly demand of end-use cluster from the total load electricity demand and explanatory variables such as temperature, solar irradiance and time features**\n",
    "\n",
    "The model is composed of a **MultiLayer perceptron for each end-use-cluster**. These MLP are trained together in order to compute the total load and compare it to the actual load. **The inputs are explanatory variables** (temperature, solar irradiance, features) and **the annual demand for the end-use cluster for each country.** How it works ? The artifical neural network finds correlation between explanatory variables and the total load to fit the actual load. \n",
    "\n",
    "## 3 - Predict results\n",
    "\n",
    "Basically predict results on the test set and assess the performance of the model. After running that cell. You can find here : **Load_curve_modelling/models/Results analysis.pynb** the whole routine to visualize graphs using the outputs of the model and save them + evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Make dataset \n",
    "\n",
    "Run the following cell if datasets are not already computed and saved to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if kernel died for no reason run this cell: \n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/data/make_dataset.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Let's have a quick look to the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df = pd.read_csv('data/processed/processed_historical_test_set.csv')\n",
    "\n",
    "\n",
    "ax1 = sns.relplot(x='hour_of_day', y='temperature',hue = 'month', kind='line', \n",
    "            data=df,palette = 'coolwarm',legend = 'full',facet_kws=dict(sharey=False),ci = None)\n",
    "ax1.fig.subplots_adjust(top=0.93)\n",
    "ax1.fig.suptitle('Temperature', fontsize = 20, fontweight = 'bold')\n",
    "ax1.set(ylim = (0))\n",
    "\n",
    "ax2 = sns.relplot(x='hour_of_day', y='irradiance_surface',hue = 'month', kind='line', \n",
    "            data=df,palette = 'coolwarm',legend = 'full',facet_kws=dict(sharey=False),ci = None)\n",
    "ax2.fig.subplots_adjust(top=0.93)\n",
    "ax2.fig.suptitle('Irradiance level', fontsize = 20, fontweight = 'bold')\n",
    "ax2.set(ylim = (0))\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(x='hour_of_day', y='services_rate', \n",
    "            data=df,label = 'services_rate')\n",
    "sns.lineplot(x='hour_of_day', y='occupation_rate', \n",
    "            data=df,label = 'occupation_rate')\n",
    "sns.lineplot(x='hour_of_day', y='activity_rate', \n",
    "            data=df,label = 'activity_rate')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1),borderaxespad=0)\n",
    "plt.title('Time dependant rate',fontsize = 20, fontweight = 'bold')\n",
    "plt.ylim(0)\n",
    "\n",
    "df.country.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Train model\n",
    "\n",
    "The trained models are saved here : **Load_curve_modelling/models/version_number** Note that you have a model trained on all countries and one folder per country. It denotes all the country specific models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the parameters of the model\n",
    "\n",
    "If you make changes, run the cells to update the parameters files.\n",
    "You can find here for each subsectors the **features associated** (e.g explanatory variables used by the model to find correlation between them and the subsector's load), and sometimes a **penalty** with other **hyper parameters**. \n",
    "\n",
    "#### 2.1.1 Irradiance penalty definition\n",
    "\n",
    "It aims to produce curves not only based on the irradiance penalty but also on the activty of services & residential. This loss aims to **add a penalty when the irradiance surface is higher than a certain threshold and when people are not active/awaken**. Because lighting is unlikely to happen when the irradiance surface is high/people are not active.  It is defined as followed : \n",
    "#### $loss = [f_{lighting}(t)(\\alpha p_{hour} + p_{lighting})]^{2}$  \n",
    "##### where :  $p_{hour} = 1 - activity$\n",
    "##### and : $p_{lighting} = \\max(0,irradiance(t) - threshold) $\n",
    "\n",
    "#### 2.1.1 Temperature penalty definition\n",
    "\n",
    "This loss aims to **add a penalty when the temperature is higher/lower than a certain temperature threshold because cooling/heating is unlikely to happen when the temperature is low/high**.  It is defined as followed : \n",
    "#### $loss = \\alpha (f_{heating/cooling}(t)\\Delta T) ^2 $  \n",
    "##### where :  $\\Delta T = \\max (0,T(t) - threshold)$ *if heating*\n",
    "##### and : $\\Delta T = \\max (0,threshold-T(t))$ *if cooling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "#The subsector mapping first : \n",
    "subsector_mapping = {\n",
    "        'clu_01': {\n",
    "            'subsectors': [\n",
    "                'RES_CK', 'SER_OT', 'RES_AP',\n",
    "                'IND_CE', 'IND_IS', 'IND_NS', 'IND_CH', 'IND_AL', 'IND_PA',\n",
    "                'IND_MI', 'IND_CO', 'IND_TR', 'IND_MA', 'IND_FO', 'IND_WO',\n",
    "                'IND_TE', 'TRA_RA', 'TRA_RO',\n",
    "            ],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'cos_w', 'sin_w','services_rate'],\n",
    "        },\n",
    "        'res_LI': {\n",
    "            'subsectors': ['RES_LI'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'irradiance_surface','activity_rate'],\n",
    "            'irradiance_penalty_res' : {'threshold': 350, 'weight' : 5} #weight corresponds to the alpha term (see above)\n",
    "            },\n",
    "        'ser_LI': {\n",
    "            'subsectors': ['SER_LI'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'irradiance_surface','services_rate'],\n",
    "            'irradiance_penalty_ser' : {'threshold': 350, 'weight' :1 } #weight corresponds to the alpha term (see above)\n",
    "            },\n",
    "        'res_SH': {\n",
    "            'subsectors': ['RES_SH', 'RES_WH'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'temperature','occupation_rate'],\n",
    "            'heating_penalty': {'threshold': 18.,\n",
    "                                'weight': 1} #weight corresponds to the alpha term (see above)\n",
    "        },\n",
    "        'ser_SH': {\n",
    "            'subsectors': ['SER_SH', 'SER_WH'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'temperature','services_rate'],\n",
    "            'heating_penalty': {'threshold': 18.,\n",
    "                                'weight': 1} #weight corresponds to the alpha term (see above)\n",
    "        },\n",
    "        'res_SC': {\n",
    "            'subsectors': ['RES_SC'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'temperature', 'cos_w', 'sin_w','occupation_rate'],\n",
    "            'cooling_penalty': {'threshold': 20.,\n",
    "                                'weight': 0.1} #weight corresponds to the alpha term (see above)\n",
    "        },\n",
    "        'ser_SC': {\n",
    "            'subsectors': ['SER_SC'],\n",
    "            'features': ['cos_h', 'sin_h', 'weekday', 'temperature', 'cos_w', 'sin_w','services_rate'],\n",
    "            'cooling_penalty': {'threshold': 20.,\n",
    "                                'weight': 0.1} #weight corresponds to the alpha term (see above)\n",
    "        }\n",
    "    }\n",
    "#The confidence interval percentage of each cluster for WEM training\n",
    "ci_ratio = {'CLU_01' : 0.3, 'CLU_LI' : 0.2 , 'CLU_SH' : 0.2 , 'CLU_SC' : 0.2}\n",
    "\n",
    "#The parameters of the neural network \n",
    "hparams = {'n_layers' : 1, #Number of layers in the LSTM (has to be done manually for the MLP)\n",
    "        'max_epochs' : 400, \n",
    "        'epochs_wem' : 0.15, # Percentage to apply on max_epochs to compute the max_epochs_wem\n",
    "        'batch_size':4,\n",
    "        'dim_hidden': 40, #Number of neurons in the hidden layer of the MLP\n",
    "        #'dim_hidden_lstm' : 2, #Number of hidden nodes in the LSTM\n",
    "        'optimizer_name': 'adam', #Optimizer of the ANN\n",
    "        'dropout_rate' : 0,\n",
    "        'lr': 0.001, #learning rate of the ANN\n",
    "        'subsector_mapping': subsector_mapping,\n",
    "        'not_use_wem_inputs' : True, #Don't touch this argument   \n",
    "        'h_ratio' : ci_ratio\n",
    "    }\n",
    "\n",
    "#Saving these parameters as configuration files : \n",
    "with open('models/logs/hparams_init.yaml','w') as yamlfile : \n",
    "    yaml.dump(hparams,yamlfile)\n",
    "with open('models/logs/subsector_mapping.yaml','w') as yamlfile : \n",
    "    yaml.dump(subsector_mapping,yamlfile)\n",
    "with open('models/logs/ci_ratio.yaml','w') as yamlfile : \n",
    "    yaml.dump(ci_ratio,yamlfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run src/models/train_model.py\n",
    "\n",
    "#if kernel died for no reason try : \n",
    "#import os \n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Predict model\n",
    "\n",
    "The prediction of the models are saved here : **Load_curve_modelling/models/version_number in excel format**. Then visualize and assess model performance using the ***Results analysis*** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the last version number of the directory\n",
    "import os\n",
    "list = os.listdir('models/logs')\n",
    "maximum = 'version_0'\n",
    "for l in list : \n",
    "    if 'version' in l : \n",
    "        if int(l.split(sep = '_')[1]) > int(maximum.split(sep = '_')[1]) : \n",
    "            maximum = l\n",
    "       \n",
    "print('Last version in the directory is',maximum)\n",
    "\n",
    "%run src/models/predict_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Connect the data of the load curve model with the World Energy Model\n",
    "\n",
    "The output is an excel file with the same format as in the WEM excel files here : *G:\\EO2021\\Model\\Sectors\\RES\\DSR\\Model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/data/connect_wem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
